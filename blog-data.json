{
  "posts": [
    {
      "id": "redis-scaling-beyond-1tb",
      "title": "Scaling Redis Clusters Beyond 1TB: Lessons from Production",
      "excerpt": "Sharing insights from scaling Redis clusters at Udemy, including performance optimization techniques and architectural decisions that enabled us to handle massive data volumes.",
      "content": "# Scaling Redis Clusters Beyond 1TB: Lessons from Production\n\n## Introduction\n\nAt Udemy, we faced the challenge of scaling our Redis infrastructure to handle over 1TB of data while maintaining sub-millisecond latency and 99.99% availability. This post shares the lessons learned from this massive scaling effort.\n\n## The Challenge\n\nOur Redis clusters were experiencing:\n- Memory pressure causing evictions\n- Increased latency during peak hours\n- Complex failover scenarios\n- Monitoring gaps at scale\n\n## Solution Architecture\n\n### 1. Cluster Sharding Strategy\n\nWe implemented a custom sharding strategy based on:\n- Consistent hashing with virtual nodes\n- Data locality optimization\n- Load balancing across availability zones\n\n```python\n# Example sharding configuration\nclass RedisClusterManager:\n    def __init__(self, nodes, replication_factor=3):\n        self.nodes = nodes\n        self.replication_factor = replication_factor\n        self.ring = self._build_consistent_hash_ring()\n    \n    def get_node_for_key(self, key):\n        hash_value = hashlib.md5(key.encode()).hexdigest()\n        return self.ring.get_node(hash_value)\n```\n\n### 2. Memory Optimization\n\n- Implemented custom eviction policies\n- Optimized data structures\n- Added compression for large values\n- Memory usage monitoring and alerting\n\n### 3. Performance Tuning\n\nKey optimizations included:\n- TCP keepalive settings\n- Connection pooling\n- Pipeline optimization\n- Custom Lua scripts for atomic operations\n\n## Results\n\n- **99.99% availability** during peak traffic\n- **Sub-millisecond latency** for 95th percentile\n- **1.2TB total capacity** across clusters\n- **Zero data loss** during failovers\n\n## Lessons Learned\n\n1. **Plan for growth**: Design with 10x capacity in mind\n2. **Monitor everything**: Custom metrics are crucial at scale\n3. **Test failure scenarios**: Chaos engineering is essential\n4. **Document everything**: Knowledge transfer is critical\n\n## Next Steps\n\nWe're exploring:\n- Redis Cluster mode migration\n- Custom memory allocators\n- Advanced monitoring with ML\n\n---\n\n*Published on December 15, 2024*",
      "date": "2024-12-15",
      "category": "Database",
      "tags": ["Redis", "Scaling", "Performance", "AWS", "Production"],
      "readTime": "8 min read",
      "author": "Nipuna Perera"
    },
    {
      "id": "kafka-connect-msk-migration",
      "title": "Migrating from Kafka Connect to MSK: A Production Guide",
      "excerpt": "A comprehensive guide on migrating Kafka Connect workloads to AWS MSK, including challenges, solutions, and best practices learned from real-world implementations.",
      "content": "# Migrating from Kafka Connect to MSK: A Production Guide\n\n## Overview\n\nThis guide covers our migration from self-managed Kafka Connect clusters to AWS MSK (Managed Streaming for Apache Kafka) at Udemy. The migration involved 50+ connectors processing 2M+ messages per day.\n\n## Why Migrate to MSK?\n\n### Benefits\n- **Reduced operational overhead**: No more cluster management\n- **Better monitoring**: CloudWatch integration\n- **Automatic scaling**: Based on usage patterns\n- **Security**: IAM integration and encryption at rest\n- **Cost optimization**: Pay for what you use\n\n### Challenges\n- **Vendor lock-in**: AWS-specific features\n- **Limited customization**: Less control over configuration\n- **Migration complexity**: Zero-downtime migration required\n\n## Migration Strategy\n\n### Phase 1: Assessment\n\n1. **Inventory existing connectors**\n2. **Analyze data flow patterns**\n3. **Identify dependencies**\n4. **Calculate resource requirements**\n\n### Phase 2: MSK Setup\n\n```yaml\n# Terraform configuration for MSK cluster\nresource \"aws_msk_cluster\" \"main\" {\n  cluster_name           = \"udemy-kafka-cluster\"\n  kafka_version          = \"2.8.1\"\n  number_of_broker_nodes = 3\n  \n  broker_node_group_info {\n    instance_type   = \"kafka.m5.large\"\n    ebs_volume_size = 1000\n    \n    client_subnets = [\n      aws_subnet.kafka_1.id,\n      aws_subnet.kafka_2.id,\n      aws_subnet.kafka_3.id\n    ]\n    \n    security_groups = [aws_security_group.kafka.id]\n  }\n  \n  encryption_info {\n    encryption_at_rest_kms_key_id = aws_kms_key.kafka.arn\n  }\n}\n```\n\n### Phase 3: Connector Migration\n\n#### Debezium Connectors\n\n```json\n{\n  \"name\": \"mysql-connector\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n    \"database.hostname\": \"mysql-cluster.internal\",\n    \"database.port\": \"3306\",\n    \"database.user\": \"debezium\",\n    \"database.password\": \"${secretsmanager:udemy/debezium:password}\",\n    \"database.server.id\": \"184054\",\n    \"database.server.name\": \"mysql\",\n    \"table.include.list\": \"udemy.courses,udemy.users\",\n    \"database.history.kafka.bootstrap.servers\": \"${kafka.bootstrap.servers}\",\n    \"database.history.kafka.topic\": \"dbhistory.mysql\",\n    \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"key.converter.schemas.enable\": false,\n    \"value.converter.schemas.enable\": false\n  }\n}\n```\n\n### Phase 4: Testing\n\n1. **Unit tests** for each connector\n2. **Integration tests** with test data\n3. **Load testing** with production-like data\n4. **Failover testing** for high availability\n\n## Monitoring and Alerting\n\n### CloudWatch Metrics\n\n```python\n# Custom CloudWatch metrics\nimport boto3\n\ncloudwatch = boto3.client('cloudwatch')\n\ndef publish_connector_metrics(connector_name, status, lag):\n    cloudwatch.put_metric_data(\n        Namespace='Udemy/KafkaConnect',\n        MetricData=[\n            {\n                'MetricName': 'ConnectorStatus',\n                'Dimensions': [\n                    {'Name': 'ConnectorName', 'Value': connector_name}\n                ],\n                'Value': 1 if status == 'RUNNING' else 0,\n                'Unit': 'Count'\n            },\n            {\n                'MetricName': 'ConnectorLag',\n                'Dimensions': [\n                    {'Name': 'ConnectorName', 'Value': connector_name}\n                ],\n                'Value': lag,\n                'Unit': 'Count'\n            }\n        ]\n    )\n```\n\n## Results\n\n### Performance Improvements\n- **40% reduction** in operational overhead\n- **99.9% uptime** for all connectors\n- **50% cost reduction** compared to self-managed\n- **Sub-second** connector restart times\n\n### Lessons Learned\n\n1. **Start small**: Migrate non-critical connectors first\n2. **Monitor closely**: Set up comprehensive alerting\n3. **Test thoroughly**: Load test with production data\n4. **Document everything**: Migration runbooks are crucial\n\n## Best Practices\n\n### Security\n- Use IAM roles for connector authentication\n- Encrypt all data in transit and at rest\n- Regular security audits\n\n### Monitoring\n- Set up CloudWatch dashboards\n- Configure appropriate alarms\n- Monitor connector lag and throughput\n\n### Maintenance\n- Regular connector updates\n- Capacity planning\n- Backup and disaster recovery\n\n---\n\n*Published on December 10, 2024*",
      "date": "2024-12-10",
      "category": "AWS",
      "tags": ["Kafka", "MSK", "Migration", "AWS", "Debezium"],
      "readTime": "12 min read",
      "author": "Nipuna Perera"
    },
    {
      "id": "database-automation-python",
      "title": "Building Database Automation with Python: A Practical Approach",
      "excerpt": "Exploring Python-based automation strategies for database operations, including monitoring, backup management, and performance optimization scripts.",
      "content": "# Building Database Automation with Python: A Practical Approach\n\n## Introduction\n\nDatabase automation is crucial for maintaining reliable, scalable systems. This post explores practical Python-based automation strategies I've developed and used in production environments.\n\n## Core Automation Areas\n\n### 1. Database Monitoring\n\n#### Health Checks\n\n```python\nimport mysql.connector\nimport redis\nimport psycopg2\nfrom datetime import datetime\nimport logging\n\nclass DatabaseHealthChecker:\n    def __init__(self, config):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n    \n    def check_mysql_health(self, host, port, user, password, database):\n        try:\n            conn = mysql.connector.connect(\n                host=host,\n                port=port,\n                user=user,\n                password=password,\n                database=database,\n                connect_timeout=5\n            )\n            \n            cursor = conn.cursor()\n            cursor.execute(\"SELECT 1\")\n            result = cursor.fetchone()\n            \n            # Check replication lag\n            cursor.execute(\"SHOW SLAVE STATUS\")\n            slave_status = cursor.fetchone()\n            \n            if slave_status and slave_status[32]:  # Seconds_Behind_Master\n                lag = slave_status[32]\n                if lag > 60:  # More than 1 minute lag\n                    self.logger.warning(f\"High replication lag: {lag} seconds\")\n            \n            cursor.close()\n            conn.close()\n            \n            return {\n                'status': 'healthy',\n                'timestamp': datetime.now().isoformat(),\n                'replication_lag': lag if 'lag' in locals() else 0\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"MySQL health check failed: {e}\")\n            return {\n                'status': 'unhealthy',\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n    \n    def check_redis_health(self, host, port, password=None):\n        try:\n            r = redis.Redis(\n                host=host,\n                port=port,\n                password=password,\n                socket_timeout=5\n            )\n            \n            # Test basic connectivity\n            r.ping()\n            \n            # Check memory usage\n            info = r.info('memory')\n            memory_usage = info['used_memory'] / info['maxmemory'] * 100\n            \n            if memory_usage > 90:\n                self.logger.warning(f\"High memory usage: {memory_usage:.2f}%\")\n            \n            return {\n                'status': 'healthy',\n                'memory_usage': memory_usage,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Redis health check failed: {e}\")\n            return {\n                'status': 'unhealthy',\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n```\n\n### 2. Automated Backups\n\n#### MySQL Backup Script\n\n```python\nimport subprocess\nimport boto3\nfrom datetime import datetime, timedelta\nimport os\nimport gzip\n\nclass MySQLBackupManager:\n    def __init__(self, config):\n        self.config = config\n        self.s3_client = boto3.client('s3')\n    \n    def create_backup(self, database, output_dir):\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        backup_file = f\"{database}_{timestamp}.sql\"\n        backup_path = os.path.join(output_dir, backup_file)\n        \n        # Create mysqldump command\n        cmd = [\n            'mysqldump',\n            f'--host={self.config[\"host\"]}',\n            f'--port={self.config[\"port\"]}',\n            f'--user={self.config[\"user\"]}',\n            f'--password={self.config[\"password\"]}',\n            '--single-transaction',\n            '--routines',\n            '--triggers',\n            '--events',\n            database\n        ]\n        \n        try:\n            # Execute backup\n            with open(backup_path, 'w') as f:\n                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)\n            \n            if result.returncode != 0:\n                raise Exception(f\"Backup failed: {result.stderr}\")\n            \n            # Compress backup\n            compressed_file = f\"{backup_path}.gz\"\n            with open(backup_path, 'rb') as f_in:\n                with gzip.open(compressed_file, 'wb') as f_out:\n                    f_out.writelines(f_in)\n            \n            # Remove uncompressed file\n            os.remove(backup_path)\n            \n            # Upload to S3\n            self.upload_to_s3(compressed_file, f\"backups/{database}/{os.path.basename(compressed_file)}\")\n            \n            # Clean up local file\n            os.remove(compressed_file)\n            \n            return {\n                'status': 'success',\n                'file': compressed_file,\n                'size': os.path.getsize(compressed_file)\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Backup failed: {e}\")\n            return {'status': 'failed', 'error': str(e)}\n    \n    def upload_to_s3(self, file_path, s3_key):\n        self.s3_client.upload_file(\n            file_path,\n            self.config['s3_bucket'],\n            s3_key,\n            ExtraArgs={'ServerSideEncryption': 'AES256'}\n        )\n    \n    def cleanup_old_backups(self, database, days_to_keep=30):\n        cutoff_date = datetime.now() - timedelta(days=days_to_keep)\n        \n        # List objects in S3\n        response = self.s3_client.list_objects_v2(\n            Bucket=self.config['s3_bucket'],\n            Prefix=f\"backups/{database}/\"\n        )\n        \n        for obj in response.get('Contents', []):\n            if obj['LastModified'].replace(tzinfo=None) < cutoff_date:\n                self.s3_client.delete_object(\n                    Bucket=self.config['s3_bucket'],\n                    Key=obj['Key']\n                )\n                self.logger.info(f\"Deleted old backup: {obj['Key']}\")\n```\n\n### 3. Performance Monitoring\n\n#### Query Performance Analyzer\n\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine, text\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass QueryPerformanceAnalyzer:\n    def __init__(self, connection_string):\n        self.engine = create_engine(connection_string)\n    \n    def analyze_slow_queries(self, hours=24):\n        query = \"\"\"\n        SELECT \n            query_time,\n            lock_time,\n            rows_sent,\n            rows_examined,\n            db,\n            sql_text,\n            user_host,\n            start_time\n        FROM mysql.slow_log \n        WHERE start_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)\n        ORDER BY query_time DESC\n        LIMIT 100\n        \"\"\"\n        \n        with self.engine.connect() as conn:\n            df = pd.read_sql(query, conn, params=[hours])\n        \n        return df\n    \n    def generate_performance_report(self, df):\n        # Top slowest queries\n        slowest_queries = df.nlargest(10, 'query_time')\n        \n        # Query time distribution\n        plt.figure(figsize=(12, 8))\n        \n        plt.subplot(2, 2, 1)\n        df['query_time'].hist(bins=50)\n        plt.title('Query Time Distribution')\n        plt.xlabel('Query Time (seconds)')\n        \n        plt.subplot(2, 2, 2)\n        df.groupby('db')['query_time'].mean().plot(kind='bar')\n        plt.title('Average Query Time by Database')\n        plt.xticks(rotation=45)\n        \n        plt.subplot(2, 2, 3)\n        df['rows_examined'].plot(kind='hist', bins=50)\n        plt.title('Rows Examined Distribution')\n        plt.xlabel('Rows Examined')\n        \n        plt.subplot(2, 2, 4)\n        df.plot(x='start_time', y='query_time', kind='scatter', alpha=0.6)\n        plt.title('Query Time Over Time')\n        plt.xlabel('Time')\n        plt.ylabel('Query Time (seconds)')\n        \n        plt.tight_layout()\n        plt.savefig('query_performance_report.png', dpi=300, bbox_inches='tight')\n        \n        return slowest_queries\n```\n\n### 4. Schema Management\n\n#### Automated Schema Changes\n\n```python\nfrom alembic import command\nfrom alembic.config import Config\nimport yaml\n\nclass SchemaManager:\n    def __init__(self, alembic_cfg_path, database_url):\n        self.alembic_cfg = Config(alembic_cfg_path)\n        self.alembic_cfg.set_main_option('sqlalchemy.url', database_url)\n    \n    def apply_migration(self, revision='head'):\n        try:\n            command.upgrade(self.alembic_cfg, revision)\n            return {'status': 'success', 'revision': revision}\n        except Exception as e:\n            return {'status': 'failed', 'error': str(e)}\n    \n    def generate_migration(self, message):\n        try:\n            command.revision(self.alembic_cfg, message=message, autogenerate=True)\n            return {'status': 'success', 'message': message}\n        except Exception as e:\n            return {'status': 'failed', 'error': str(e)}\n    \n    def rollback_migration(self, revision):\n        try:\n            command.downgrade(self.alembic_cfg, revision)\n            return {'status': 'success', 'revision': revision}\n        except Exception as e:\n            return {'status': 'failed', 'error': str(e)}\n```\n\n## Automation Best Practices\n\n### 1. Error Handling\n\n```python\nimport logging\nfrom functools import wraps\n\ndef retry_on_failure(max_retries=3, delay=1):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_retries - 1:\n                        logging.error(f\"Function {func.__name__} failed after {max_retries} attempts: {e}\")\n                        raise\n                    logging.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n                    time.sleep(delay * (2 ** attempt))  # Exponential backoff\n            return None\n        return wrapper\n    return decorator\n```\n\n### 2. Configuration Management\n\n```python\nimport yaml\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass DatabaseConfig:\n    host: str\n    port: int\n    user: str\n    password: str\n    database: str\n    ssl_mode: str = 'prefer'\n\nclass ConfigManager:\n    def __init__(self, config_file):\n        with open(config_file, 'r') as f:\n            self.config = yaml.safe_load(f)\n    \n    def get_database_config(self, env='production') -> DatabaseConfig:\n        db_config = self.config['databases'][env]\n        return DatabaseConfig(**db_config)\n    \n    def get_automation_config(self) -> Dict:\n        return self.config['automation']\n```\n\n### 3. Monitoring and Alerting\n\n```python\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\nclass AlertManager:\n    def __init__(self, smtp_config):\n        self.smtp_config = smtp_config\n    \n    def send_alert(self, subject, message, recipients):\n        msg = MIMEMultipart()\n        msg['From'] = self.smtp_config['from']\n        msg['To'] = ', '.join(recipients)\n        msg['Subject'] = subject\n        \n        msg.attach(MIMEText(message, 'plain'))\n        \n        with smtplib.SMTP(self.smtp_config['host'], self.smtp_config['port']) as server:\n            server.starttls()\n            server.login(self.smtp_config['user'], self.smtp_config['password'])\n            server.send_message(msg)\n```\n\n## Conclusion\n\nDatabase automation with Python provides powerful capabilities for maintaining reliable, scalable systems. The key is to start small, focus on the most critical operations, and gradually expand your automation coverage.\n\nRemember:\n- **Test thoroughly** before deploying to production\n- **Monitor everything** and set up appropriate alerts\n- **Document your automation** for team knowledge sharing\n- **Keep it simple** - complexity is the enemy of reliability\n\n---\n\n*Published on December 5, 2024*",
      "date": "2024-12-05",
      "category": "Automation",
      "tags": ["Python", "MySQL", "Redis", "Automation", "Monitoring"],
      "readTime": "15 min read",
      "author": "Nipuna Perera"
    }
  ],
  "categories": [
    {"name": "Database", "count": 1},
    {"name": "AWS", "count": 1},
    {"name": "Automation", "count": 1}
  ],
  "tags": [
    {"name": "Redis", "count": 1},
    {"name": "Scaling", "count": 1},
    {"name": "Performance", "count": 1},
    {"name": "AWS", "count": 1},
    {"name": "Production", "count": 1},
    {"name": "Kafka", "count": 1},
    {"name": "MSK", "count": 1},
    {"name": "Migration", "count": 1},
    {"name": "Debezium", "count": 1},
    {"name": "Python", "count": 1},
    {"name": "MySQL", "count": 1},
    {"name": "Automation", "count": 1},
    {"name": "Monitoring", "count": 1}
  ]
}
